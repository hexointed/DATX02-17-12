\chapter{Discussion} 

	This chapter will discuss our thoughts on our result, our theoretical not
	yet	implemented optimizations, and what future work is possible from this
	point in time.
	
	\section{Development Environment}
		
		In this section we describe some project specific tools and our experience using them.
		
		\subsection{\clash}
			As described in \ref{FHDLs}, the ability to simulate the function of
			the GPU directly in a REPL was completely essential in us being able
			to complete a working GPU in the short time span available. The way
			\clash{} handles state is quite elegant and effective once one has
			learned the general pattern. Lists however, are too abstract for
			synthesis so \clash{}  uses its own vector type instead. Most 
			regular
			data functions are implemented, but all such vectors must have a
			fixed size. This makes some operations such as combining two vectors
			rather cumbersome and one has to work around this.
	
		\subsection{Synthclipse}
			This is an IDE for developing shaders, using GLSL or JSX. It has
			features for rapid recompilation and testing, easy resource loading 
			and	Uniform Controls. The Uniform Controls are a collection of user
			interface elements such as sliders and color pickers, which can be
			saved into Presets. These allowed us to navigate our scene and 
			modify its parameters on the fly, enabling visual examination and 
			debugging of our algorithms.		
		
		\section{Software Shader}

			The optimizations that were implemented surprised with the enormous performance increase they gave. There are many optimizations that are yet to be
			implemented and tested and it's hard to estimate the potential
			achievable performance of a shader on conventional GPU. The only thing that is certain is that
			there are massive performance potentials of a shader on conventional hardware. To develop a GLSL shader for algorithmic development and testing was a great decision as it helped us understand the algorithm and how to improve it.
			
			GLSL has some very attractive features, it was easy to learn 
			because the syntax has much in common with C++. This makes the time it takes 
			to develop new features and	test them short. The purpose of the 
			software shader was to study the algorithm, implement and test 
			optimizations and develop new features. For the purposes of this 
			project GLSL was a perfect fit.
		
		\section{GPU} 
		
			The project originally intended to create a simple single-core GPU
			that could fully render a scene given enough time, and then add	
			components as time allowed. Every design decision one makes 
			clarifies what further needs to be done, and reveals previously 
			unseen flaws in the design that must
			be resolved. In the end we implemented the core and a storage 
			manager which enabled multiple cores running in parallel.

			The storage manager turned out to need to be more complex than we 
			originally anticipated. In its first implementation, the storage in
			the storage manager was a simple FIFO queue. This worked well for 
			very simple shader programs, but we found that it was not possible
			to write a full Sphere Tracer using this storage type. This was not
			due to any complexity inherent in the sphere tracing algorithm, but
			rather a consequence of how the GPU requires the shader programs to
			keep track of all pixels they need to render. This is solved by 
			having the shader generate a new thread for every pixel it needs to
			render, and then running the actual pixel shader code on that 
			thread. This fails when threads need to execute many jumps or 
			loops, which are also solved by spawning new threads. Whenever a 
			thread executed a jump, the pixel spawning thread took priority 
			because of the FIFO storage, and a new pixel thread would be 
			spawned. After many jumps, the storage in the storage manager would 
			fill up entirely, and all cores would stall waiting for the 
			storage manager allowing them to send it more threads. This was 
			solved by changing the internal storage to a double-ended queue,
			giving the threads some control over execution priority.

			If we were to continue working on this project another workflow
			would be adopted when developing the GPU. More detailed planning is
			required in the early stages and empiric testing of different
			implementations is required along the entire development to reach
			the best possible solution.

	\section{Optimizations} \label{optimization}

		\subsection{Bounding spheres}

			If used the right way this technique can give great performance 
			boosts. The testing showed an increase in gain as the number of 
			objects increased inside the sphere, which is promising.

			The test was in this case simpler than the issue. The results depend
			on more factors than we can analyze in our time frame, the test only handles the 
			number of objects inside a fixed sphere. To be used in a optimal way
			the objects should be bound in a way similar to the one described in
			the subsection implementation\ref{implshader} but more adaptive, 
			enclosing objects depending how they are grouped per frame. It should also not try to compute the sphere definition exactly but rather aproximate it for increased performance. 

		\subsection{Orthogonal culling}

			The performance gain from this optimization was surprisingly high. However great the performance gain, this
			is still far from the optimal implementation of Orthogonal culling. 
			It currently performs Orthogonal Culling on all the objects in 
			the scene, per pixel. 
			Instead, culling should be done once per object and calculate 
			which rays an object intersects, this way fewer calculations would
			have to be performed and optimally this should be performed by the
			CPU. 

			This method could work together with the Bounding Sphere
			optimization. Instead of projecting single objects onto the ray,
			bounding spheres could be culled, decreasing the number of
			orthogonal projections that has to be made. This was tested and
			an increase in performance was observed.
			
		\subsection{Ray grouping}

			Ray grouping improved performance by lowering the total number of
			marching steps by decreasing the total numer of initial rays. The
			improvements seemed to be more significant for higher resolutions,
			which is reasonable, given that the cost at each step for a given
			group is not dependant on the number of rays in the group. This 
			could be very useful when rendering on high-resolution displays.
			
			We had initially expcted the performance improvements to be greater
			than those acheived. There is, however, a significant amount of
			overhead incurred from splitting groups of rays into smaller
			groups, which is the likely cause of both the decreased
			improvements as well as the diminishing returns when increasing the
			group size to a significant portion of the display size.

			Ray grouping was also the only optimization that was implemented on
			the GPU. The hardware support for threads and the ability to
			creating new threads dynamically worked very well with this
			optimization. This optimization also reduces the amount of rays
			that have similar execution flows and might therefore be less
			effective on traditional lockstepped architectures, although it
			should probably still be possible to implement ray grouping and
			improve performance on these GPUs.

	\section{Square roots}

		The simple square root approximator and its improved version are
		extremely simple circuits, in fact they are significantly smaller than
		most other common operations performed on numbers in hardware. Of
		course, they are far from accurate, but we find it interesting that it
		is possible to achieve a bounded relative error from the true square
		root for any number of input bits with these small circuits. When
		combined with linear interpolation or iterative methods like the
		Babylonian method the accuracy of these approximations increases
		significantly.
		
		The shifting nth root algorithm is the only bit-accurate method tested,
		and it performs well, albeit slower than the approximations. If
		non-exact results are acceptable, which they often are to some extent
		when doing sphere tracing, the number of steps in the shifting nth root
		algorithm could be reduced, which would both increase speed and
		decrease area usage. The lerp-based and iterative methods might still
		be able to reuse common components such as adders and multipliers
		however, and might therefore be preferable.
		
		For the GPU, we have used the shifting nth root algorithm in order
		to not have to consider possible accuracy problems when debugging
		shaders, but all methods are implemented and ready to be switched out
		at any time.
		

	\section{Future work} \label{futureoptimizations} 

		\subsection{GPU-CPU cooperation}

			In the current implementation everything is performed on the GPU,
			including object transformations and culling. Currently if objects
			in the scene are supposed to move they have to be moved using
			mathematical functions, such as sinus. These functions are then
			evaluated for each march step. This is essentially wasted computing
			power and could be performed on the CPU as is standard in modern
			graphics engines.

		\subsection{Plane-ray intersection}

			Infinite planes currently require a lot of computing power to
			render. When the camera is oriented so that the field of view is
			along an infinite plane some rays will travel parallel to the
			plane. These rays will march equally long steps until their max
			number of steps has been taken or the max range has been reached, 
			without hitting anything. This will cause some rays to draw 
			computing power without ever being able to hit the plane anyway. 
			Some rays that should hit the plane will fail to do so because of 
			to many steps taken, making it look like the plane has an edge 
			because after a certain range it is no longer being rendered.

			One potential problem is that because no distance field evaluations 
			are performed, it might be hard to perform Boolean operations or 
			mathematical deformations on planes rendered this way. Another is 
			that every time we add more features that aren't part of the 
			regular Sphere Tracing, the complexity of the program increases. 
			Thus, the advantages of the individual optimizations must be 
			weighed against the overall performance loss they incur. 

		\subsection{Overstepping}

			Bounding spheres technique is somewhat similar to the normal sphere
			tracing. The difference is that when you march you march to the edge of
			the MDS and then you march to the next MDS edge. With overstepping
			technique you march a small distance further outside the MDS edge. You
			then compare the original MDS with the new MDS if these two spheres
			overlap in any way we can march that little bit further. By marching that
			little bit further, a decrease in the number of times marched is achieved,
			giving an increase in performance. This has previously been discussed
			by\cite{Korndorfer2014}. If the two MDS's do not overlap it steps back to 
			the edge of the first MDS, not overstepping, then continuing like before.
			

		
			
			

	\section{Future impact on industry} 

		The GPU is still too underdeveloped to definitively show if the
		algorithm has potential one way or another.  Further prototypes needs
		to be built with greater rigor before that can happen. If we however
		assume that somewhere down the line it turns out to be a success,
		then there are generally three ways in which we could foresee that
		happening.
		
		First up is by breaking through in the games industry. Granted that
		it probably will not be able to outperform modern graphic cards at
		their own game even after improving the card as much as possible. But
		the up-and-coming VR technology faces different challenges, usually
		rendering smaller scenes with fewer objects and capturing actual
		``3D’’ images to display to the user. Both of these factors favors
		Sphere Tracing since it inherently produces perfect 3D objects and
		works much faster in scenes with shorter render distance.
		
		Another possibility is that the GPU turns out to be too slow for
		rendering games in real time, but runs other Ray Tracing algorithms
		commonly used in movie production faster that modern graphic cards.
		This is not too unlikely since the calculation of the rays in both
		cases follow many similar patterns in bouncing and reflecting etc.
		Since movies render in render farms where like server farms, lots of
		copies of the same hardware works in tandem. Meaning that even a
		small improvement over current graphic cards might be amplified to a
		significant gain in computational power.
		
		Finally, we designed our GPU similar to a CPU but with more	cores
		like a GPU except with no lockstepping. If this proves to be feasible
		for a full ASIC (Application-specific integrated circuit), then this
		type of processing unit would be better at parallel processing than a
		regular CPU, and it would perform better than a regular GPU at
		recursive and conditionally branching algorithms. This lends itself
		well to computations such as fractals, population simulation and
		artificial intelligence. It might also fit classes of computation not
		yet thought of, such in the example of Folding@Home\cite{Beberg2009}.
		They created software, deployed all over the world, to run on
		Graphics cards in home computers, when the computer was idle. So
		instead of wasting precious computing power, these GPUs, adept at
		parallel computations, were repurposed to work on the complicated
		problem of protein folding. In effect acting as a global
		supercomputer to aid medical research. Given the uniqueness of our
		GPU it might find a similar novel use in the future.
