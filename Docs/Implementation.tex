\chapter{Implementation}

	In order to achieve the goals set in this project, two separate pieces of
	software and one piece of hardware were written. The software programs are
	two different implementations of a reference shader. The first of these was
	written in GLSL\cite{glsl} for traditional graphics hardware, and the second
	is written in an assembly language we designed for the Sphere Tracing
	processor. Both of these shaders are described in section \ref{implshader}.
	The hardware is comprised of a multi-core programmable parallel processing
	unit designed for the Sphere Tracing algorithm, detailed in section
	\ref{implproc}.
	
	\section{Shaders} \label{implshader}

		The shaders written uses the theory described in chapter
		\ref{spheretracing}, iterating over a ray in their main loop. The GLSL
		shader has more optimizations, but lacks the ray grouping optimization. The
		optimizations are described separately after the shaders.

		\subsection{GLSL Based}
	
			This shader is designed to run on conventional graphics hardware. It was
			created in the high-level shading language GLSL (short for OpenGL Shading
			Language). It executes its main-loop for each pixel the shader is
			rendered on, using the screen-coordinates as input. The Bounding Sphere
			and Ortogonal Culling optimizations were implemented and used in this
			shader.
			
			Before the loop is run Orthogonal Culling is executed, reducing the
			number of objects in the distance field. Orthogonal Culling can cull
			individual objects or Bounding Spheres. The Sphere Tracing loop is a
			straight-forward implementation of the algorithm.
			
			After a hit is detected different things can happen depending on the
			material assigned to the object. If the object is reflective, then the
			normal of the surface that was hit is calculated and used to reflect the
			ray. If the object is colored the pixel will simply be set according to
			the object. Color can also be calculated from object or ray coordinates.

		\subsection{Assembler Based}

			In this shader the Ray Grouping optimization was implemented. This shader
			was written in an assembly language we designed for our GPU. When this
			shader starts executing, a single thread is initially created. This
			thread will create a single render thread and one new version of itself
			in order to create more render threads until it has created a render
			thread for each pixel (or group if Ray Grouping is used). This will
			happen continiously during execution. This will render all pixels as
			concurrently as the GPU has cores, minimizing core stalls and therefore
			maximizing GPU usage.

			When a render thread is created it immediately starts setting up the ray.
			The screen coordinates are calculated from the thread id, this is
			essential for calculating the ray direction. Camera position and a point
			to orient the camera against are predefined. 

			After screen coordinates are calculated a direction vector for the ray is
			calculated. The Sphere Tracing loop is a straight-forward implementation
			of the algorithm. If a hit is detected a color will be assigned to the
			corresponding pixel.

		\subsection{Bounding Spheres Optimization}
			
			In order to decrease the number of distance calculations that has 
			to be performed in each march step, objects can be grouped together 
			in larger enclosing bounding spheres. When the distance field is 
			evaluated only the bounding sphere will be considered and none of 
			the objects enclosed by	it. If the bounding sphere is hit by the 
			ray, it will be removed and all	the contained objects will be 
			introduced to the distance field.
			
			A Bounding Sphere is defined by its location, radius and enclosed
			objects. The location is the mean position of all the enclosed objects.
			The radius is made large enough to encompass all of it's objects.

			In our implementation a Bounding Sphere is simply an object in the
			scene. Instead of returning a color or redefining the ray (such 
			as reflecting objects do) when hit it will return the smallest 
			distance of the enclosed objects. Our implementation does allow for
			nested Bounding Spheres but this was not tested.

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.75\linewidth]{figure/BoundingSphere} 
				\caption{Bounding spheres, notice that the distance to individual objects 
					is not calculated until the ray is inside a sphere.}
			\end{figure}

		\subsection{Orthogonal Culling Optimization}

			A bottleneck of Sphere Tracing is that the distance to a lot of objects
			has to be calculated once for every march step. Performance can be
			increased by reducing the number of objects that the distance has to be
			calculated to, without reducing the actual number of objects in the
			scene. To do this the GPU needs to calculate which object a ray might hit
			during the execution of the Sphere Tracing algorithm. This information is
			traditionally calculated on the CPU and then passed to the GPU (a.k.a
			Frustum culling\cite{frustum}).

			Orthogonal Culling works by going through all the objects in the scene
			and calculating what objects a ray might hit. This is done for each ray
			before the Sphere Tracing loop begins. Orthogonal projection is used to
			tell whether an object is in front of the ray or not. By projecting the
			center of the bounding sphere of the object onto the ray, the line's
			closest point to the object is known. Then, the distance between the line
			and the sphere can be calculated using the radius of the bounding sphere.
			If this distance is less than zero, the ray will intersect the sphere.
			This way the number of objects that has to be evaluated at each march
			step is reduced. 

			\begin{figure}[H]
                \centering
                \includegraphics[width=0.75\linewidth]{figure/WithCulling} 
                \caption{Note that the objects that are not on the path of the ray
                is not considered at every step.}
            \end{figure}

		\subsection{Ray Grouping Optimization}

			Ray Grouping works by grouping adjacent rays together and marching them
			all at once in a single step. Each group of rays marches along its center
			pixels' ray in the same way that is done in normal Sphere Tracing. For
			each step, the smallest distance to the scene is compared to the total
			size of the group, to check if any ray might collide with the geometry.
			If any ray of the group might be inside the geometry, the group will be
			split into four new groups. Each new group then repeats these steps again
			individually along its new center. This is repeated either until a group
			has stepped far enough into the scene to discard the group entirely or
			until there is only one ray left in the group, and that ray has hit an
			object.  The closer the groups are to a target, the more likely it is for
			a higher amount of sub-groups to be created due to the decreased distance
			to the geometry, making less pixels fit inside the group. When a group is
			far from an object in the scene it will likely not need to split and
			can therefore reduce the number of computations. An illustration of the reduced number of rays is shown in figure \ref{raybunch}.

			\begin{figure}[H]
                \centering
                \includegraphics[width=0.75\linewidth]{figure/raybunching} 
								\caption{A simulated $64 \cross 64$ display. Black pixels are
								rays that hit an object. Green pixels are groups that were
								discarded because they did not hit any objects. Note that the
								groups are larger further from the sphere. White pixels did not
								need to be calculated because of the ray grouping. }
								\label{raybunch}
            \end{figure}

	\section{Processor Unit} \label{implproc}

		The properties of the ray marching algorithm as described in chapter
		\ref{spheretracing} led to the decision of designing a GPU architecture
		that does not utilize multi-ALU SIMD with lockstepping to the same
		extent as that of traditional graphics processors. We instead went for
		a solution with slightly simpler independent cores. This is similar to
		the reasoning employed by \cite{Woop2005} for a Ray Tracing hardware
		design.

		The hardware described in this section was written in \clash. \clash is
		based on the functional programming language Haskell and is therefore a
		functional hardware description language, or FHDL.

		\subsection{Architecture}

			The GPU is designed to be able to handle a form of threading
			natively, where all computations are performed by a thread running
			on a hardware core. A thread in this context is very light-weight:
			only an instruction pointer along with some mutable registers are
			stored.

			All threads that are not running are sent to a storage manager that
			communicates with all cores. Threads are then dispatched to the 
			cores as they become available for execution. The cores are also 
			able to spawn new threads by sending their data to the storage 
			manager, or by terminating their current thread. This is done 
			either by simply dropping them or by sending them to the frame 
			buffer, where the register content will be used to determine the 
			color of a pixel on the screen.

			The storage in the storage manager on the GPU is implemented
			as a double-ended queue. This makes it possible for threads to have
			some coarse-grained control over execution priority, which is
			needed in order for many types of programs to be able to run with a
			bounded number of threads at any given time. During execution the
			storage manager selects one core that is ready to send a thread, if
			any core is ready to do so, and adds its thread to the storage. If
			any core is ready to receive a thread, one is taken from the 
			storage and sent to that core. If more than one core is ready, one
			is chosen arbitrarily.

			Each core has an instruction memory and a data memory. The
			instruction memory controls logic for execution order and the DFU.
			The data and instruction memories are immutable and mirrored across
			all cores in their entirety. The control logic is responsible for
			feeding the DFU with instructions and data. It also updates the
			thread registers when a new thread is attached, sending thread
			registers to the storage manager when new threads are spawned and
			requesting a new thread whenever the previously executing one
			terminates.
			
			The DFU contains a stack which all arithmetic instructions operate
			on, as well as an ALU for carrying out calculations. It also keeps
			track of all the thread registers and updates them when such
			instructions are issued. The DFU contains very little extra control
			logic and is only able to receive and execute instructions, it does
			not keep track of any instruction pointers or when threads are
			attached or dropped, this is instead handled by the core control
			logic.

		\subsection{Instruction Set}

			A representation for distance functions was also designed. It is
			implemented as a reverse polish notation inspired stack based instruction
			set. There are a total of four instruction types with different
			instruction encodings. Their layout in memory is shown in figure
			\ref{encodingfig} and the type of instructions are:

			\begin{figure}
				\centering
				\caption{Bit encoding for the different instruction types. All
					instruction types have a fixed width 16-bit encoding.}
				\label{encodingfig}
				\includegraphics[width=0.75\linewidth]{figure/pdf/instruction_layout} 
			\end{figure}

			\begin{description} 
				\item[C-type] instructions are for control flow, that
						is, instructions that affect the global queue or frame buffer or
						terminate the current thread. C-type instructions can be
						conditionally executed based on the condition encoded in the two
						highest bits in the opcode, equal to zero or not equal to zero. 

				\item[D-type] instructions encode arithmetic- and vector
					operations that work on the stack. These operate only on the
					internal stack in the core and not on any of the other
					registers. They can take up to six arguments and return up
					to three return values. D-type instructions can be split up
					in chunks using the \texttt{next} instruction, where the
					last computed result will be accumulated using the min-function
					automatically. This is useful for encoding distance functions.

				\item[V-type] instructions deal with memory accesses. The
					\texttt{p} bit encodes whether this access is for pack
					(mutable register) or read-only memory.

				\item[R-type] instructions load wide immediate values into the
					DFU. They are currently only used for assigning distance
					function IDs.
			\end{description}

		\subsection{Execution Model}

			The processor contains a global storage structure that is shared
			among all cores by a single storage manager. All threads that are 
			ready for execution are held in this storage until a core is ready 
			to start executing them.

			\begin{figure}[H]
				\centering
				\caption{ GPU Data flow diagram }
					\includegraphics[width=0.75\linewidth]{figure/dots/GPU-schematic} 
				\vspace{-4pt}
			\end{figure}
	
			Each shader thread has 16 mutable registers that are automatically
			loaded into the core when execution starts. These registers are
			saved when execution pauses and copied whenever a thread spawns a 
			new child. The instruction \texttt{setval} can be used to change 
			the value of a register, and the instruction \texttt{pack} reads 
			the value of a register.

			All calculations are performed with intermediate values stored on a
			stack, with the ability to move any values between the stack and
			any of the registers using the \texttt{setval} and \texttt{pack}
			instructions.

			There are three main instructions for control flow: \texttt{pushq},
			\texttt{pushf}, and \texttt{drop}. Threads can spawn new threads at
			any time using the instruction \texttt{pushq}: this will cause the
			core to make a copy of all of the registers and push it onto the
			global queue, where another (or the same) core may later access
			them and start executing from the instruction pointer register
			(\texttt{r0}).

			Threads can also terminate execution in two ways. Executing the
			instruction \texttt{drop} will terminate the thread and discard all
			registers. Executing \texttt{pushf} will terminate the thread and
			discard all values except the pixel pointer (\texttt{r1}) and color
			register (\texttt{r2}), which will be sent to the frame buffer in
			order to be displayed on the screen.

			This set of instructions for control flow might initially seem to be
			neither useful nor easy to implement but it is powerful enough for all
			branching that is needed in our Sphere Tracer while being restrictive
			enough to make instruction memory accesses very predictable.

			The instructions \texttt{next} and \texttt{accum} are used to accelerate
			finding the minimum distance for a set of distance functions using a
			built-in accumulator. Because of this, distance functions can be written
			without any regard for how the shader that called them works. This very
			common operation being automated thus reduces the size of each distance
			function slightly.

		\subsection{Square Roots}

			Calculating distances is an integral part of the algorithm, so a
			good square root implementation was considered important for
			achieving good performance. Several different approaches for this
			were considered.

			Firstly, using an iterative method like Goldschmidt's or the 
			Babylonian
			method require an initial starting value which is usually generated
			using a look-up table. We tested a different and very fast method for
			generating an initial rough guess of the square root of any number
			without the need for a look-up table. A 16-bit version of this circuit is
			shown in figure \ref{orsqrt}. The basic working principle is
			based on the fact that the square root of a number has roughly half as
			many digits as the original number. An improved but slightly more expensive
			version is showed in figure \ref{orsqrt2}. In this version with slightly improved
			accuracy the placement of the extra wires is based on the bit pattern of
			$\sqrt{2}$.

			\begin{figure}
				\centering
				\caption{A simple square root approximator}
				\label{orsqrt}
				\includegraphics[width=0.75\linewidth]{figure/pdf/simpleOr.pdf} 
			\end{figure}

			\begin{figure}
				\centering
				\caption{Simple square root approximator with minor improvement.
					More layers of or-gates can be added with increased shifts 
					for slight increases in accuracy. }
				\label{orsqrt2}
				\includegraphics[width=0.75\linewidth]{figure/pdf/sqrt3Or.pdf} 
			\end{figure}

			In order to improve upon this method both in terms of accuracy and
			stability another design was considered. The input was both rounded up
			and down to values where the simple square root approximator was very
			accurate and the result was then linearly interpolated between these
			roots, as seen in the circuit in figure \ref{orsqrt3}. This linear interpolation
			requires only a multiplication and a couple of additions, but choosing
			upper and lower seed values is a more complex problem. Choosing the two
			geometrically closest powers of two was tested because the or-gate square
			root circuit is very accurate for powers of two, but this is not the only
			possible choice, as there are many points for which the simple square
			root approximator is very accurate. We did not look further into choosing
			different upper and lower bounds for the lerp-approximator.

			\begin{figure}
				\centering
				\caption{ Simple square root approximator used together with
					linear interpolation. This version includes an extra layer
					of or-gates with a shift of 4 and is more accurate for
					powers of 2. Note that the gates in this diagram seem to be
					ordered in two extra layers. This is, however, only a
					visual artifact in this image. Adding more layers with
					shifts of 6, 10, 14, 26 etc increases accuracy further but 
					with rapidly diminishing returns.}
				\label{orsqrt3}
				\includegraphics[width=0.75\linewidth]{figure/pdf/sqrt2Or.pdf} 
			\end{figure}

			Another algorithm for calculating square roots is the
			\emph{Shifting nth root algorithm}, also known as \emph{Dijkstras
			square root algorithm}\cite{Li}, which calculates the square root one digit
			at a time. A \texttt{c} implementation of this algorithm is shown
			in figure \ref{sqrtc}. Initially, this algorithm does not seem like
			a good candidate for a high performance hardware implementation,
			but several important optimizations are possible when unrolling it
			combinatorially. Most of the additions can be already reduced to bitwise
			logic operations when computing it in software, as shown in
			figure \ref{sqrtcopt}. In a combinatorial hardware implementation,
			these bitwise operations can be removed entirely by simply moving
			wires around yielding a logic cost of 0. The only remaining
			operations is therefore the subtraction on line 6, and the
			conditional assignments of \texttt{num} and \texttt{res} on lines 9
			and 10. The cost of the subtraction can be significantly reduced
			because \texttt{res} approximates the result one bit every step,
			meaning most bits of \texttt{num} and \texttt{res} are zeroed at
			any given iteration. This means that on average, only 25\% of the
			subtraction logic is needed. This turned out to result in a design
			very similar to the PARSQRT implementation by \cite{Li}.

			\begin{table}
				\centering
				\caption{A \texttt{C} implementation of the shifting nth root 
					algorithm. The shifting nth root algorithm approximates the
					square root one bit at a time, and is as accurate as the 
					precision allows. All calculated bits are stored in the 
					variable \texttt{res}.}
				\label{sqrtc}
				\begin{tabular}{c}
				\begin{lstlisting}
int isqrt(int num) {
    int res = 0;
    int bit = 1 << 30;

    while(bit != 0) {
        if(num >= res + bit) {
            num -= res + bit;
            res = (res >> 1) + bit;
        } else res >>= 1;
        bit >>= 2;
    }
    return res;
}
				\end{lstlisting}
				\end{tabular}
			\end{table}

			\begin{table}
				\centering
				\caption{A \texttt{C} implementation of the shifting nth root
					algorithm. This code example is slightly longer, but
					relatively expensive additions have been replaced by
					bitwise ors. The bit-shift of \texttt{res} has been moved, 
					removing the else-statement, and the comparison has been 
					rewritten to require less logic when implemented in 
					hardware.}
				\label{sqrtcopt}
				\begin{tabular}{c}
				\begin{lstlisting}
int isqrt(int num) {
    int res = 0;
    int bit = 1 << 30;

    while(bit != 0) {
        int tmp = num - (res | bit);
        res >>= 1;
        if(tmp >= 0) {
            num = tmp;
            res |= bit;
        }
        bit >>= 2;
    }
    return res;
}
				\end{lstlisting}
				\end{tabular}
			\end{table}

